# Part 1: Process Fundamentals

---

## Theory Questions

**Q1.** What is a Process?
> **Answer:** A process is a program in execution. It is an active entity that requires resources such as CPU, memory, files, and I/O devices to accomplish its task.

---

**Q2.** What are the key characteristics of a Process?
> **Answer:**
> - **Dynamic Nature**: A process is a running instance of a program
> - **Resource Consumer**: Requires CPU time, memory, files, and I/O devices
> - **Unique Identity**: Each process has a unique Process ID (PID)
> - **Independent Execution**: Processes execute independently with their own memory space

---

**Q3.** What are the components of a Process?
> **Answer:**
> 1. **Text Section (Code Segment)**: Contains executable code, read-only, shared among multiple instances
> 2. **Data Section**: Contains global and static variables (initialized and uninitialized)
> 3. **Heap**: Dynamically allocated memory, grows upward, used for malloc/new
> 4. **Stack**: Contains temporary data (function parameters, return addresses, local variables), grows downward, LIFO structure
> 5. **Program Counter (PC)**: Points to the next instruction to execute
> 6. **CPU Registers**: Store current values of variables and intermediate results

---

**Q4.** Draw and explain the memory layout of a process.
> **Answer:**
> ```
> High Memory Address
> +------------------+
> |      Stack       | ← Grows Downward
> |        ↓         |
> +------------------+
> |   Free Memory    |
> +------------------+
> |        ↑         |
> |      Heap        | ← Grows Upward
> +------------------+
> |   Data Section   |
> | (Global/Static)  |
> +------------------+
> |   Text Section   |
> |     (Code)       |
> +------------------+
> Low Memory Address
> ```

---

**Q5.** Differentiate between Process and Program.
> **Answer:**
> | Aspect | Program | Process |
> |--------|---------|---------|
> | **Nature** | Passive entity (static) | Active entity (dynamic) |
> | **Lifespan** | Exists permanently | Temporary, exists only during execution |
> | **Resource Usage** | Does not consume resources | Consumes CPU, memory, I/O |
> | **Location** | Stored on disk | Loaded in memory |
> | **State** | No state | Has various states (new, ready, running, etc.) |
> | **Example** | `.exe` file on disk | Running instance of `.exe` |

---

**Q6.** Give an example to explain Program vs Process.
> **Answer:**
> - **Program**: `notepad.exe` stored on your hard drive
> - **Process**: When you double-click `notepad.exe`, multiple running instances become separate processes
> - **Important**: One program can create multiple processes. For example, opening Chrome multiple times creates multiple Chrome processes.

---

**Q7.** What is Process Control Block (PCB)?
> **Answer:** The Process Control Block (PCB) is a data structure maintained by the operating system for every process. It contains all the information needed to manage and control a process. It is also known as Task Control Block (TCB) or Process Descriptor.

---

**Q8.** What are the components of PCB?
> **Answer:**
> 1. **Process Identification**: Process ID (PID), Parent Process ID (PPID), User ID
> 2. **Process State**: Current state (New, Ready, Running, Waiting, Terminated)
> 3. **Program Counter**: Address of the next instruction to be executed
> 4. **CPU Registers**: Accumulator, Index Registers, Stack Pointers, General Purpose Registers
> 5. **CPU Scheduling Information**: Priority, Scheduling Queue Pointers, Time quantum
> 6. **Memory Management Information**: Base and Limit Registers, Page Tables, Segment Tables
> 7. **Accounting Information**: CPU Time Used, Time Limits, Process Start Time, Real Time Used
> 8. **I/O Status Information**: List of Open Files, I/O Devices Allocated, List of I/O Requests

---

**Q9.** What is the importance of PCB?
> **Answer:**
> 1. **Process Management**: OS uses PCB to track and manage all processes
> 2. **Context Switching**: PCB stores process state during context switches
> 3. **Multiprogramming**: Enables multiple processes to share CPU
> 4. **Process Scheduling**: Contains information needed for scheduling decisions
> 5. **Resource Allocation**: Tracks resources allocated to each process

---

**Q10.** Explain the Five-State Process Model.
> **Answer:**
> 1. **NEW (Created)**: Process is being created, PCB is being initialized, resources are being allocated
> 2. **READY**: Process has all resources except CPU, loaded in main memory, waiting in Ready Queue
> 3. **RUNNING (Executing)**: Process currently being executed by CPU, only one process per CPU core can be in this state
> 4. **WAITING (Blocked)**: Process waiting for some event (I/O completion, user input, signal)
> 5. **TERMINATED (Exit)**: Process has finished execution, resources being deallocated, PCB being removed

---

**Q11.** What is the Ready Queue?
> **Answer:** Ready Queue is a queue data structure that holds all processes ready for execution. These are processes that have all resources except CPU and are waiting to be assigned to a processor.

---

**Q12.** What are the characteristics of RUNNING state?
> **Answer:**
> - Process is actually using the CPU
> - Instructions are being executed
> - Only one process per CPU core can be in RUNNING state
> - In multi-core systems, multiple processes can run simultaneously (one per core)
> - A process stays in running state until it completes execution, time quantum expires, requires I/O, or higher priority process arrives

---

**Q13.** What are the characteristics of WAITING state?
> **Answer:**
> - Cannot execute even if CPU is available
> - Waiting for external events: I/O completion, User input, Signal from another process, Resource availability
> - Stored in Wait Queue or Device Queue
> - Multiple wait queues exist (one per event type)
> - Example: Program waiting for user to input data, process waiting for file from disk

---

**Q14.** What are the reasons for Process termination?
> **Answer:**
> - **Normal Exit**: Process completed successfully
> - **Error Exit**: Process encountered an error
> - **Fatal Error**: Serious error (division by zero, illegal instruction)
> - **Killed by Another Process**: Terminated by parent or OS

---

**Q15.** Explain all the Process State Transitions.
> **Answer:**
> | Transition | From State | To State | Trigger Event |
> |------------|------------|----------|---------------|
> | **Admitted** | NEW | READY | OS admits process into ready queue |
> | **Dispatch** | READY | RUNNING | Scheduler selects process for execution |
> | **Interrupt** | RUNNING | READY | Time quantum expires or higher priority process arrives |
> | **I/O or Event Wait** | RUNNING | WAITING | Process requests I/O or waits for event |
> | **I/O or Event Completion** | WAITING | READY | I/O operation completes or event occurs |
> | **Exit** | RUNNING | TERMINATED | Process completes execution |

---

**Q16.** What is Context Switching?
> **Answer:** Context Switch is the process of saving the state of a currently running process and restoring the state of another process to resume its execution.

---

**Q17.** What are the steps involved in Context Switch?
> **Answer:**
> 1. **Save Current Process State**: Save CPU registers to PCB, Save Program Counter value, Save process state as READY or WAITING
> 2. **Update PCB**: Update process state, Update accounting information
> 3. **Move PCB**: Move current process to appropriate queue, Select next process from Ready queue
> 4. **Restore New Process State**: Load CPU registers from PCB of new process, Load Program Counter, Change process state to RUNNING
> 5. **Resume Execution**: CPU starts executing the new process

---

**Q18.** What is Context Switch Time? What factors affect it?
> **Answer:**
> - **Pure Overhead**: No useful work is done during context switch
> - **Typical Duration**: Few milliseconds (1-10 ms)
> 
> **Factors Affecting Context Switch Time:**
> 1. **Hardware Support**: Multiple register sets, special context switch instructions
> 2. **Number of Registers**: More registers = more time to save/restore
> 3. **Memory Speed**: Faster memory = quicker save/restore operations
> 4. **Process Complexity**: More resources = more state to save

---

**Q19.** When does Context Switch occur?
> **Answer:**
> 1. **Interrupt Handling**: Hardware or software interrupt
> 2. **Multitasking**: Time quantum expires in time-sharing systems
> 3. **User and Kernel Mode Switch**: Transition between modes
> 4. **I/O Operations**: Process waits for I/O completion
> 5. **Priority Changes**: Higher priority process becomes ready

---

**Q20.** Differentiate between Context Switch and Mode Switch.
> **Answer:**
> | Aspect | Context Switch | Mode Switch |
> |--------|----------------|-------------|
> | **Definition** | Switching between processes | Switching between user and kernel mode |
> | **Scope** | Complete process state saved | Only privilege level changes |
> | **Overhead** | High (several microseconds) | Low (minimal) |
> | **PCB Change** | Yes | No |
> | **Example** | Switching from Browser to Music Player | System call execution |

---

**Q21.** Can a process in WAITING state transition directly to RUNNING state?
> **Answer:** No, a process in WAITING state cannot transition directly to RUNNING state. It must first transition to READY state (when the event it was waiting for completes), and then the scheduler can dispatch it to RUNNING state.

---

**Q22.** Why is only one process per CPU core allowed in RUNNING state?
> **Answer:** Only one process per CPU core can be in RUNNING state because a single CPU core can only execute one instruction stream at a time. The CPU physically processes instructions sequentially from one process. However, in multi-core systems, multiple processes can run simultaneously (one per core).

---

# Part 2: Threads & Multithreading

---

**Q23.** What is a Thread?
> **Answer:** A thread is the smallest unit of execution within a process. It is a lightweight process that can execute independently while sharing the same process resources.

---

**Q24.** What are the key characteristics of a Thread?
> **Answer:**
> - **Lightweight**: Threads are lighter than processes
> - **Shared Resources**: All threads of a process share the same memory space
> - **Independent Execution**: Each thread has its own execution path
> - **Part of a Process**: Threads cannot exist independently; they belong to a process

---

**Q25.** Why do we need Threads?
> **Answer:** Traditional processes have a single thread of execution. Modern applications need to perform multiple tasks simultaneously:
> - Web browser: Download files, render pages, run JavaScript simultaneously
> - Word processor: Type text, check spelling, auto-save concurrently
> - Media player: Decode audio, display video, handle user input at the same time

---

**Q26.** What are the components of a Thread?
> **Answer:** Each thread has its own:
> 1. **Thread ID**: Unique identifier for the thread within the process
> 2. **Program Counter (PC)**: Points to the next instruction to execute
> 3. **Register Set**: Thread-specific CPU registers for working variables
> 4. **Stack**: Thread-local stack for function calls, local variables, return addresses

---

**Q27.** What do Threads share from the Parent Process?
> **Answer:** Threads within the same process share:
> 1. **Code Section (Text Segment)**: All threads execute the same program code
> 2. **Data Section**: Global variables and static variables
> 3. **Heap**: Dynamically allocated memory
> 4. **Operating System Resources**: Open files, I/O devices, Signals

---

**Q28.** Differentiate between Process and Thread.
> **Answer:**
> | Aspect | Process | Thread |
> |--------|---------|--------|
> | **Definition** | Independent program in execution | Lightweight unit of execution within a process |
> | **Memory** | Separate memory space | Shares memory with other threads |
> | **Communication** | IPC (expensive) | Direct access to shared memory (cheap) |
> | **Creation Time** | Slower (heavyweight) | Faster (lightweight) |
> | **Context Switch** | Expensive (more overhead) | Cheaper (less overhead) |
> | **Isolation** | Completely isolated | Not isolated (share resources) |
> | **Termination** | Terminates all threads | Only that thread terminates |
> | **Resources** | Independent resources | Shares process resources |
> | **Example** | Two instances of Chrome | Multiple tabs in one Chrome instance |

---

**Q29.** What are the advantages of Threads over Processes?
> **Answer:**
> 1. **Faster Creation**: Thread creation is 10-100 times faster than process creation
> 2. **Faster Termination**: Thread cleanup is quicker
> 3. **Faster Context Switching**: Less state to save/restore
> 4. **Efficient Communication**: Shared memory eliminates need for IPC
> 5. **Resource Sharing**: Automatic sharing of code, data, and files

---

**Q30.** What are the disadvantages of Threads?
> **Answer:**
> 1. **No Protection**: One thread can corrupt another thread's data
> 2. **Debugging Complexity**: Race conditions and synchronization bugs are harder to debug
> 3. **Process Crash**: If one thread crashes, entire process may crash
> 4. **No Isolation**: Security vulnerabilities in one thread affect all threads

---

**Q31.** What are the benefits of Multithreading?
> **Answer:**
> 1. **Responsiveness**: Application remains responsive even during long operations
> 2. **Resource Sharing**: Threads automatically share process memory and resources
> 3. **Economy**: Creating and managing threads is cheaper than processes
> 4. **Scalability**: Process can take advantage of multiprocessor/multicore systems

---

**Q32.** Explain how a Multi-Threaded Web Server is better than a Single-Threaded Server.
> **Answer:**
> **Single-Threaded Server**: Clients must wait for previous client's request to complete
> ```
> Client 1 request → Process → Response
>                    ↓ (must wait)
> Client 2 request → Process → Response
> ```
> 
> **Multi-Threaded Server**: Multiple clients served concurrently
> ```
> Client 1 request → Thread 1 → Response
> Client 2 request → Thread 2 → Response (concurrent)
> Client 3 request → Thread 3 → Response (concurrent)
> ```

---

**Q33.** What are User-Level Threads (ULT)?
> **Answer:** User-Level Threads are threads managed by user-level thread library without kernel involvement. The kernel is unaware of these threads.
> 
> **Characteristics:**
> - Thread library manages all thread operations
> - Thread table maintained in user space
> - Kernel sees only a single-threaded process
> - Examples: POSIX Pthreads (in user mode), Mach C-threads, Solaris threads

---

**Q34.** What are the advantages of User-Level Threads?
> **Answer:**
> 1. **Fast Thread Operations**: Creation, switching, termination are faster, no kernel mode switch required
> 2. **Portable**: Can be implemented on any OS, does not require kernel support
> 3. **Customizable Scheduling**: Application can implement its own scheduling algorithm
> 4. **Scalable**: Can create thousands of threads, not limited by kernel's thread limit

---

**Q35.** What are the disadvantages of User-Level Threads?
> **Answer:**
> 1. **Blocking System Calls**: If one thread makes a blocking system call, entire process blocks
> 2. **No True Parallelism**: Cannot take advantage of multicore systems
> 3. **Limited Scheduling**: Thread cannot be preempted by kernel
> 4. **Page Fault Problem**: Page fault in one thread blocks entire process

---

**Q36.** What are Kernel-Level Threads (KLT)?
> **Answer:** Kernel-Level Threads are threads managed directly by the operating system kernel. The kernel is fully aware of all threads.
> 
> **Characteristics:**
> - Kernel maintains thread table
> - All thread operations require system calls
> - Kernel schedules threads, not processes
> - Examples: Windows threads, Linux kernel threads (NPTL)

---

**Q37.** What are the advantages of Kernel-Level Threads?
> **Answer:**
> 1. **True Parallelism**: Different threads can run on different CPU cores simultaneously
> 2. **No Blocking Problem**: If one thread blocks on I/O, other threads continue execution
> 3. **Better Responsiveness**: Kernel can preempt threads
> 4. **Kernel Routines Can Be Multithreaded**: Kernel itself can use threads

---

**Q38.** What are the disadvantages of Kernel-Level Threads?
> **Answer:**
> 1. **Slow Thread Operations**: Thread creation, switching require system calls
> 2. **Limited Scalability**: Creating thousands of threads puts burden on kernel
> 3. **Operating System Dependent**: Implementation varies across operating systems

---

**Q39.** Compare User-Level Threads and Kernel-Level Threads.
> **Answer:**
> | Feature | User-Level Threads | Kernel-Level Threads |
> |---------|-------------------|---------------------|
> | **Management** | Thread library | Operating system kernel |
> | **Kernel Awareness** | Kernel unaware | Kernel aware |
> | **Speed** | Fast (no system calls) | Slow (system calls required) |
> | **Context Switch** | Very fast | Slower |
> | **Blocking Calls** | Blocks entire process | Blocks only that thread |
> | **Multicore Support** | No true parallelism | True parallelism |
> | **Scalability** | Highly scalable | Limited by kernel resources |
> | **Portability** | Portable | OS-dependent |

---

**Q40.** What is the Many-to-One Multithreading Model?
> **Answer:** Many user-level threads are mapped to a single kernel thread.
> 
> **Characteristics:**
> - Multiple user threads managed by thread library
> - Kernel sees only one kernel thread
> - Thread management done entirely in user space
> 
> **Advantages:** Efficient thread management, fast operations, no kernel support needed
> 
> **Disadvantages:** One blocking system call blocks all threads, no parallelism
> 
> **Examples:** Green threads (older Java), GNU Portable Threads

---

**Q41.** What is the One-to-One Multithreading Model?
> **Answer:** Each user thread is mapped to a separate kernel thread.
> 
> **Characteristics:**
> - Each user thread has corresponding kernel thread
> - Kernel manages all threads
> - One-to-one mapping
> 
> **Advantages:** True concurrency on multiple CPUs, no blocking issue
> 
> **Disadvantages:** Creating user thread requires creating kernel thread, limited threads
> 
> **Examples:** Windows, Linux (NPTL), Modern Java threads

---

**Q42.** What is the Many-to-Many Multithreading Model?
> **Answer:** Many user threads are multiplexed to a smaller or equal number of kernel threads (M user threads mapped to N kernel threads, M ≥ N).
> 
> **Advantages:**
> - Best of both worlds
> - Can create many user threads (scalability)
> - Multiple kernel threads provide parallelism
> - No blocking problem
> 
> **Disadvantages:** Complex implementation, coordination overhead
> 
> **Examples:** Older Solaris, HP-UX, Tru64 UNIX

---

**Q43.** What is the Two-Level Model?
> **Answer:** A variation of Many-to-Many model that allows both many-to-many and one-to-one mappings. Some user threads can be "bound" to specific kernel threads.
> 
> **Advantages:** Flexibility, critical tasks get guaranteed kernel support
> 
> **Examples:** Solaris (with bound threads), IRIX, HP-UX

---

**Q44.** What are the five Thread States?
> **Answer:**
> 1. **NEW**: Thread is being created but not yet started
> 2. **READY**: Thread is ready to run, waiting for CPU
> 3. **RUNNING**: Thread is currently executing
> 4. **BLOCKED**: Thread is waiting for event (I/O, lock, etc.)
> 5. **TERMINATED**: Thread has finished execution

---

**Q45.** What are the Thread Operations?
> **Answer:**
> 1. **Thread Creation**: Create a new thread within the process
> 2. **Thread Termination**: End thread execution (normal termination, explicit exit, cancellation, process termination)
> 3. **Thread Join**: Wait for another thread to complete (calling thread blocks until target terminates)
> 4. **Thread Yield**: Voluntarily give up CPU to another thread (moves from RUNNING to READY)

---

**Q46.** Why is thread creation faster than process creation?
> **Answer:** Thread creation is faster because:
> - No need to allocate separate memory space (threads share parent process memory)
> - No need to duplicate code section, data section, or heap
> - Only need to allocate thread-specific resources (stack, registers, PC)
> - Less initialization overhead

---

**Q47.** Why do User-Level Threads suffer from the blocking problem?
> **Answer:** In ULT, the kernel is unaware of threads - it only sees the process. When any thread makes a blocking system call (like I/O), the kernel blocks the entire process (not just that thread). Since the kernel doesn't know about individual threads, it cannot schedule another thread from the same process during the block.

---

# Part 3: CPU Scheduling

---

**Q48.** What is CPU Scheduling?
> **Answer:** CPU Scheduling is the process by which the operating system decides which process in the ready queue should be executed next by the CPU. The main objective is to make the system efficient, fast, and fair by maximizing CPU utilization and minimizing wait time.

---

**Q49.** What is the CPU & I/O Burst Cycle?
> **Answer:** Processes alternate between two states:
> 1. **CPU Burst**: The process is executing instructions on the CPU
> 2. **I/O Burst**: The process is waiting for I/O (disk, network, user input)
> 
> CPU-bound processes have long CPU bursts. I/O-bound processes have short CPU bursts and frequent I/O waits.

---

**Q50.** What is Preemptive Scheduling?
> **Answer:** The OS can interrupt a currently running process and move it to the Ready queue.
> 
> **Triggers:** Timer interrupt (time slice expired), higher priority process arrives
> 
> **Algorithms:** SRTF, LRTF, Round Robin, Priority based
> 
> **Pros:** Better responsiveness, prevents one process from hogging CPU
> 
> **Cons:** Overhead of context switching, data consistency issues

---

**Q51.** What is Non-Preemptive Scheduling?
> **Answer:** Once a process gets the CPU, it keeps it until it voluntarily releases it (terminates or waits for I/O). The OS cannot interrupt the process.
> 
> **Algorithms:** FCFS, SJF, LJF, HRRN, Multilevel Queue
> 
> **Pros:** Low overhead (fewer context switches), simple
> 
> **Cons:** Poor response time, "Convoy Effect"

---

**Q52.** What are the CPU Scheduling Criteria (Performance Metrics)?
> **Answer:**
> 1. **CPU Utilization** (Maximize): Percentage of time CPU is busy working
> 2. **Throughput** (Maximize): Number of processes completed per unit time
> 3. **Turnaround Time (TAT)** (Minimize): Total time from submission to completion
>    - `TAT = Completion Time - Arrival Time` = `Burst Time + Waiting Time`
> 4. **Waiting Time (WT)** (Minimize): Total time spent waiting in Ready queue
>    - `WT = Turnaround Time - Burst Time`
> 5. **Response Time** (Minimize): Time from submission until first response

---

**Q53.** What is FCFS (First-Come, First-Served) Scheduling?
> **Answer:** A non-preemptive scheduling algorithm where the process requesting the CPU first is allocated the CPU first. It operates like a real-life queue.
> 
> **Criteria:** Arrival Time
> **Mode:** Non-Preemptive
> 
> **Pros:** Very easy to understand
> 
> **Cons:** Convoy Effect - if a large process arrives first, all smaller processes must wait

---

**Q54.** What is the Convoy Effect?
> **Answer:** Convoy Effect happens in FCFS scheduling when a large CPU-bound process arrives first and holds the CPU, causing all smaller I/O-bound processes behind it to wait, significantly reducing system efficiency.

---

**Q55.** What is SJF (Shortest Job First) Scheduling?
> **Answer:** A scheduling algorithm that associates with each process the length of its next CPU burst. The CPU is assigned to the process with the smallest next CPU burst.
> 
> **Criteria:** Burst Time
> **Mode:** Non-Preemptive
> 
> **Pros:** Gives minimum average waiting time (Optimal)
> 
> **Cons:** Difficult to predict burst time, Starvation of long processes

---

**Q56.** What is SRTF (Shortest Remaining Time First) Scheduling?
> **Answer:** The preemptive version of SJF. If a new process arrives with a CPU burst less than the remaining time of the current executing process, the kernel preempts the current process.
> 
> **Criteria:** Burst Time
> **Mode:** Preemptive
> 
> **Pros:** Extremely efficient for short tasks
> 
> **Cons:** Long tasks keep getting interrupted

---

**Q57.** What is Priority Scheduling?
> **Answer:** A scheduling algorithm where a priority number is associated with each process. The CPU is allocated to the process with the highest priority.
> 
> **Mode:** Can be Preemptive or Non-Preemptive
> 
> **Issue:** Starvation - low priority processes may never execute
> 
> **Solution:** Aging - gradually increasing priority of waiting processes

---

**Q58.** What is Aging in CPU Scheduling?
> **Answer:** Aging is a technique to prevent starvation in Priority Scheduling. It involves gradually increasing the priority of processes that wait for a long time, ensuring they eventually get a chance to execute.

---

**Q59.** What is Round Robin (RR) Scheduling?
> **Answer:** A preemptive scheduling algorithm designed for time-sharing systems. Each process is given a small unit of CPU time (time quantum), and the ready queue is treated as a circular queue.
> 
> **Criteria:** Time Quantum
> **Mode:** Preemptive
> 
> **Pros:** Fair scheduling, no process waits indefinitely
> 
> **Cons:** Higher turnaround time than SJF, context switching overhead
> 
> **Note:** If time quantum is too large, it behaves like FCFS. If too small, excessive context switching overhead.

---

**Q60.** What is Multilevel Queue Scheduling?
> **Answer:** A scheduling algorithm that partitions the ready queue into several separate queues (e.g., foreground and background). Processes are permanently assigned to one queue, and each queue has its own scheduling algorithm.
> 
> **Example:**
> - System Processes (Highest Priority) → Round Robin
> - Interactive Processes → SJF
> - Batch Processes (Lowest Priority) → FCFS
> 
> **Cons:** Lower priority queues might suffer starvation

---

**Q61.** What is Multilevel Feedback Queue Scheduling?
> **Answer:** A scheduling algorithm that allows processes to move between queues. It separates processes according to their CPU burst characteristics.
> 
> **Working:** Process starts in highest priority queue. If it uses too much CPU time, it moves to lower priority queue.
> 
> **Pros:** Most flexible scheduling algorithm
> 
> **Cons:** Most complex to implement

---

**Q62.** What is LJF (Longest Job First) Scheduling?
> **Answer:** A non-preemptive scheduling algorithm that selects the process with the longest execution time (largest CPU burst) from the ready queue.
> 
> **Pros:** Reduces context switching for large processes
> 
> **Cons:** Smaller processes may face starvation

---

**Q63.** What is LRTF (Longest Remaining Time First) Scheduling?
> **Answer:** The preemptive version of LJF. The CPU constantly checks if there is a process with a longer remaining time than the current one and switches to it if found.
> 
> **Cons:** High overhead and not practical for general systems

---

**Q64.** What is HRRN (Highest Response Ratio Next) Scheduling?
> **Answer:** A non-preemptive scheduling algorithm designed to correct the starvation problem of SJF. It calculates priority based on both waiting time and service time.
> 
> **Response Ratio = (Waiting Time + Burst Time) / Burst Time**
> 
> **Pros:** Prevents starvation (Aging is built-in)
> 
> **Cons:** Higher calculation overhead

---

## Numerical Questions (CPU Scheduling)

**Q65.** Given the following processes, calculate Completion Time, Turnaround Time, and Waiting Time using FCFS:

| Process | Arrival Time | Burst Time |
| :--- | :--- | :--- |
| P1 | 0 | 2 |
| P2 | 1 | 2 |
| P3 | 5 | 3 |
| P4 | 6 | 4 |

> **Answer:**
> **Gantt Chart:**
> ```
> |  P1  |  P2  | Idle |  P3  |  P4  |
> 0      2      4      5      8      12
> ```
> 
> | Process | AT | BT | CT | TAT (CT-AT) | WT (TAT-BT) |
> | :--- | :--- | :--- | :--- | :--- | :--- |
> | P1 | 0 | 2 | 2 | 2 | 0 |
> | P2 | 1 | 2 | 4 | 3 | 1 |
> | P3 | 5 | 3 | 8 | 3 | 0 |
> | P4 | 6 | 4 | 12 | 6 | 2 |
> 
> **Average TAT** = (2+3+3+6)/4 = 3.5 units

---

**Q66.** Given the following processes, calculate using SJF (Non-Preemptive):

| Process | Arrival Time | Burst Time |
| :--- | :--- | :--- |
| P1 | 1 | 3 |
| P2 | 2 | 4 |
| P3 | 1 | 2 |
| P4 | 4 | 4 |

> **Answer:**
> **Gantt Chart:**
> ```
> | Idle |  P3  |  P1  |  P2  |  P4  |
> 0      1      3      6      10     14
> ```
> At Time 1: P1 (BT=3) and P3 (BT=2) arrive. P3 has smaller BT, executes first.
> At Time 6: P2 and P4 have same BT (4), FCFS used - P2 arrived first.
> 
> | Process | AT | BT | CT | TAT | WT |
> | :--- | :--- | :--- | :--- | :--- | :--- |
> | P1 | 1 | 3 | 6 | 5 | 2 |
> | P2 | 2 | 4 | 10 | 8 | 4 |
> | P3 | 1 | 2 | 3 | 2 | 0 |
> | P4 | 4 | 4 | 14 | 10 | 6 |

---

**Q67.** Given the following processes, calculate using SRTF (Preemptive):

| Process | Arrival Time | Burst Time |
| :--- | :--- | :--- |
| P1 | 0 | 5 |
| P2 | 1 | 3 |
| P3 | 2 | 4 |
| P4 | 4 | 1 |

> **Answer:**
> **Gantt Chart:**
> ```
> | P1 |  P2  | P4 |  P1  |  P3  |
> 0    1      4    5      9      13
> ```
> - Time 0: P1 starts
> - Time 1: P2 arrives (BT=3 < P1 remaining=4). P1 preempted.
> - Time 4: P2 finishes. P4 arrives (BT=1). P4 is shortest.
> - Time 5: P4 finishes. P1 remaining=4, P3 remaining=4. P1 resumes (FCFS).
> 
> | Process | AT | BT | CT | TAT | WT | RT |
> | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
> | P1 | 0 | 5 | 9 | 9 | 4 | 0 |
> | P2 | 1 | 3 | 4 | 3 | 0 | 0 |
> | P3 | 2 | 4 | 13 | 11 | 7 | 7 |
> | P4 | 4 | 1 | 5 | 1 | 0 | 0 |

---

**Q68.** Calculate using Round Robin (Time Quantum = 2) for:

| Process | Arrival Time | Burst Time |
| :--- | :--- | :--- |
| P1 | 0 | 5 |
| P2 | 1 | 4 |
| P3 | 2 | 2 |
| P4 | 4 | 1 |

> **Answer:**
> **Gantt Chart:**
> ```
> | P1 | P2 | P3 | P1 | P4 | P2 | P1 |
> 0    2    4    6    8    9    11   12
> ```
> **Ready Queue Evolution:**
> - 0-2: P1 (Rem:3). Queue: P2, P3, P1
> - 2-4: P2 (Rem:2). Queue: P3, P1, P4(arr@4), P2
> - 4-6: P3 (Rem:0). Finishes. Queue: P1, P4, P2
> - 6-8: P1 (Rem:1). Queue: P4, P2, P1
> - 8-9: P4 (Rem:0). Finishes. Queue: P2, P1
> - 9-11: P2 (Rem:0). Finishes. Queue: P1
> - 11-12: P1 (Rem:0). Finishes.
> 
> | Process | AT | BT | CT | TAT | WT | RT |
> | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
> | P1 | 0 | 5 | 12 | 12 | 7 | 0 |
> | P2 | 1 | 4 | 11 | 10 | 6 | 1 |
> | P3 | 2 | 2 | 6 | 4 | 2 | 2 |
> | P4 | 4 | 1 | 9 | 5 | 4 | 4 |

---

**Q69.** In Multilevel Feedback Queue, if a process P1 requires 19 units of Burst Time and the queues have Time Quantums of 2, 4, 8, and FCFS, trace P1's execution.
> **Answer:**
> 1. **RQ1 (TQ=2)**: P1 runs for 2 units. Remaining = 17. Not finished → Moved to RQ2.
> 2. **RQ2 (TQ=4)**: P1 runs for 4 units. Remaining = 13. Not finished → Moved to RQ3.
> 3. **RQ3 (TQ=8)**: P1 runs for 8 units. Remaining = 5. Not finished → Moved to RQ4.
> 4. **RQ4 (FCFS)**: P1 runs for 5 units. Remaining = 0. **Finished**.

---

**Q70.** Why is Round Robin better for a time-sharing system than FCFS?
> **Answer:** Round Robin is better because:
> - Each process gets a fair share of CPU time
> - No process has to wait indefinitely (bounded waiting)
> - Quick response time for interactive users
> - FCFS would make interactive users wait for long batch jobs to complete (Convoy Effect)

---

**Q71.** If the Time Quantum is extremely small, what is the impact on the system?
> **Answer:** If the Time Quantum is extremely small:
> - Context switches occur very frequently
> - Most of the CPU time is spent on context switching overhead rather than actual process execution
> - System performance degrades significantly
> - Throughput decreases

---

# Part 4: Process Synchronization

---

**Q72.** What is Process Synchronization?
> **Answer:** Process Synchronization is the task of coordinating the execution of processes in a way that no two processes can access the same shared data and resources at the same time effectively and efficiently.

---

**Q73.** What are Independent and Cooperative Processes?
> **Answer:**
> - **Independent Processes**: Execution does not affect others
> - **Cooperative Processes**: Execution can affect or be affected by other processes. They share Variables, Memory, Code, Resources (CPU, Printer, Scanner)

---

**Q74.** What is a Race Condition?
> **Answer:** A Race Condition is a situation where several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place.

---

**Q75.** Explain Race Condition with an example.
> **Answer:** Consider `int shared = 5;` with P1 doing `shared++` and P2 doing `shared--`:
> 
> | Process P1 | Process P2 |
> | :--- | :--- |
> | `int x = shared;` (Read 5) | `int y = shared;` (Read 5) |
> | `x++;` (x becomes 6) | `y--;` (y becomes 4) |
> | `shared = x;` (Write 6) | `shared = y;` (Write 4) |
> 
> If interleaved: P1 reads 5, P2 reads 5, P2 writes 4, P1 writes 6.
> **Result:** Final value is 6, but should be 5 (5+1-1). P2's update is lost.

---

**Q76.** What is the Critical Section Problem?
> **Answer:** The Critical Section is a code segment where shared variables, common files, and global variables are accessed or modified. The Critical Section Problem is ensuring that when one process is executing in its critical section, no other process can enter its critical section.

---

**Q77.** What is the structure of a Process with Critical Section?
> **Answer:**
> ```c
> main() {
>     // Non-Critical Section
>     
>     ENTRY SECTION      // Request permission
>     
>         CRITICAL SECTION   // Access Shared Resources
>         
>     EXIT SECTION       // Release permission
>     
>     // Remainder Section
> }
> ```

---

**Q78.** What are the four conditions required for a Synchronization Mechanism?
> **Answer:**
> 1. **Mutual Exclusion (Mandatory)**: If process P1 is in critical section, P2 cannot enter
> 2. **Progress (Mandatory)**: If CS is empty and some processes want to enter, only those in entry section can participate in the decision. A non-interested process cannot stop an interested process.
> 3. **Bounded Waiting (Optional/Fairness)**: There must be a limit on how many times other processes enter their CS after a process has made a request. Prevents starvation.
> 4. **No Assumption related to Hardware Speed**: Solution should work regardless of CPU speed or number of processors.

---

**Q79.** What is Strict Alternation? What is its problem?
> **Answer:** Strict Alternation uses a shared variable `turn` to alternate between two processes.
> - Process 0 enters if `turn == 0`
> - Process 1 enters if `turn == 1`
> 
> **Problem:** Satisfies Mutual Exclusion but **Violates Progress**.
> If P0 enters and exits (sets turn=1), and P0 wants to enter again (but P1 is not interested), P0 is blocked by its own turn variable waiting for P1.

---

**Q80.** Explain Peterson's Solution.
> **Answer:** Peterson's Solution is a classic software solution that guarantees Mutual Exclusion, Progress, and Bounded Waiting for two processes.
> 
> **Concept (The Polite Protocol):**
> 1. **Flag (Desire)**: "I raise my hand." `Flag[i] = True` means process i wants to enter.
> 2. **Turn (Politeness)**: "After you." `Turn = j` means process i gives priority to j.
> 
> **Algorithm:**
> ```c
> flag[i] = true;          // I want to enter
> turn = j;                // I give priority to you
> while (flag[j] == true && turn == j);  // Wait if other wants AND it's their turn
>     CRITICAL SECTION
> flag[i] = false;         // I am done
> ```

---

**Q81.** What is a Semaphore?
> **Answer:** A Semaphore is an integer variable that is used in a mutually exclusive manner by various concurrent processes to achieve synchronization. It can only be accessed through two standard atomic operations: `wait()` (P/down) and `signal()` (V/up).

---

**Q82.** Explain the wait() and signal() operations on Semaphore.
> **Answer:**
> **wait() / P() / down():** Request access to a resource (Entry Section)
> ```c
> wait(S) {
>     while (S <= 0);  // Busy wait
>     S--;
> }
> ```
> 
> **signal() / V() / up():** Release a resource (Exit Section)
> ```c
> signal(S) {
>     S++;
> }
> ```

---

**Q83.** What is a Binary Semaphore (Mutex)?
> **Answer:** A semaphore that can only have values 0 or 1.
> 
> **Characteristics:**
> - Also called Mutex Lock
> - Value range: {0, 1}
> - wait(S) → Lock (S becomes 0)
> - signal(S) → Unlock (S becomes 1)
> 
> **Use Cases:** Protecting critical sections, implementing mutual exclusion

---

**Q84.** What is a Counting Semaphore?
> **Answer:** A semaphore that can have any non-negative integer value.
> 
> **Characteristics:**
> - Value range: 0 to ∞
> - Can manage multiple instances of a resource
> 
> **Example:** If 5 printers are available, semaphore initialized to 5. Each wait() decrements (printer allocated), each signal() increments (printer released).

---

**Q85.** What is the Producer-Consumer Problem?
> **Answer:** A classical synchronization problem where two processes, the Producer and Consumer, share a common, fixed-size buffer and must be synchronized to avoid race conditions.
> 
> - **Producer**: Produces data items and places them into buffer
> - **Consumer**: Consumes (removes) data items from buffer
> - **Shared Resources**: Buffer[0...n-1], in (write pointer), out (read pointer), count

---

**Q86.** Why does the naive Producer-Consumer solution fail?
> **Answer:** The operations `count = count + 1` and `count = count - 1` are not atomic. At machine level, they break into multiple steps (Load, Increment/Decrement, Store). If interleaved, one process's update can be lost, causing data inconsistency.

---

**Q87.** How is Producer-Consumer solved using Semaphores?
> **Answer:** Use three semaphores:
> 1. **S (Binary)**: Init 1 (Mutual Exclusion)
> 2. **Empty (Counting)**: Init N (Empty slots)
> 3. **Full (Counting)**: Init 0 (Filled slots)
> 
> **Producer:** `down(Empty)`, `down(S)`, produce, `up(S)`, `up(Full)`
> 
> **Consumer:** `down(Full)`, `down(S)`, consume, `up(S)`, `up(Empty)`

---

**Q88.** What is the Readers-Writers Problem?
> **Answer:** A synchronization problem where:
> - **Readers**: Only read shared data
> - **Writers**: Read and write (modify) shared data
> 
> **Access Rules:**
> - Multiple Readers can access simultaneously (R-R is safe)
> - Only ONE Writer can access at a time (exclusive)
> - R-W, W-R, W-W are all problematic

---

**Q89.** Why are R-W, W-R, and W-W problematic in Readers-Writers?
> **Answer:**
> - **R-W**: Reader may read partial/inconsistent data while writer modifies
> - **W-R**: Data modification during read causes dirty read
> - **W-W**: Lost updates, data corruption, race conditions

---

**Q90.** How is Readers-Writers solved using Semaphores?
> **Answer:**
> **Variables:**
> - `int rc = 0` (Reader Count)
> - `Semaphore mutex = 1` (Protects rc)
> - `Semaphore db = 1` (Controls database access)
> 
> **Reader:** First reader locks db, last reader unlocks db
> 
> **Writer:** Simply acquires exclusive access with down(db)

---

**Q91.** What is Writer Starvation in Readers-Writers?
> **Answer:** The basic solution may lead to writer starvation if readers keep arriving. As long as at least one reader is in critical section, new readers can enter. A steady stream of readers could indefinitely block waiting writers.

---

**Q92.** What is the Dining Philosophers Problem?
> **Answer:** N philosophers sit around a circular table with a bowl of rice and N forks between them. A philosopher needs two forks (left and right) to eat. The challenge is to prevent deadlock, starvation, and ensure mutual exclusion.

---

**Q93.** What is the Deadlock scenario in Dining Philosophers?
> **Answer:** If all philosophers become hungry simultaneously and each picks up their left fork:
> - P0 picks F0, P1 picks F1, P2 picks F2, P3 picks F3, P4 picks F4
> - Now each needs their right fork: P0 needs F1 (held by P1), P1 needs F2 (held by P2), etc.
> - This creates circular wait - no philosopher can proceed or release their fork.

---

**Q94.** What is a Monitor?
> **Answer:** A Monitor is a high-level synchronization construct that provides a convenient mechanism for process synchronization. It encapsulates shared data, procedures that operate on the data, and synchronization between concurrent processes.
> 
> **Key Property:** Only ONE process can be active inside the monitor at any time (automatic mutual exclusion).

---

**Q95.** What are the problems with Semaphores that Monitors solve?
> **Answer:**
> - Programmer must remember correct order of wait() and signal()
> - Easy mistakes: Deadlock (wait, wait), No Mutual Exclusion (signal before wait), Forgetting to call signal() or wait()
> - Monitors provide automatic mutual exclusion

---

**Q96.** What are Condition Variables in Monitors?
> **Answer:** Condition variables allow processes to wait for specific conditions within a monitor.
> 
> **Operations:**
> - `x.wait()`: Suspends calling process and releases monitor lock
> - `x.signal()`: Wakes up ONE waiting process. If no one is waiting, signal has no effect.
> 
> **Difference from Semaphore:** Semaphore signal() always increments; Monitor signal() has no effect if no one is waiting.

---

**Q97.** What is the difference between Hoare and Mesa Semantics for Monitors?
> **Answer:**
> | Type | Behavior |
> |------|----------|
> | **Hoare Semantics** | Signaling process immediately suspends; awakened process runs |
> | **Mesa Semantics** | Signaling process continues; awakened process moves to ready queue |
> 
> Mesa requires using `while` instead of `if` for condition checks.

---

**Q98.** What is Message Passing?
> **Answer:** Message Passing is an IPC mechanism where processes communicate by explicitly sending and receiving messages, without sharing memory.
> 
> **Basic Operations:**
> - `send(destination, message)`
> - `receive(source, message)`

---

**Q99.** What is the difference between Direct and Indirect Communication in Message Passing?
> **Answer:**
> | Aspect | Direct | Indirect |
> |--------|--------|----------|
> | **Addressing** | Processes explicitly name each other | Messages sent to/from mailboxes |
> | **Flexibility** | Low (must know receiver) | High (mailbox abstraction) |
> | **Coupling** | Tight | Loose |
> | **Example** | Pipes | Message Queues |

---

**Q100.** What is Blocking vs Non-Blocking in Message Passing?
> **Answer:**
> **Blocking (Synchronous):**
> - Blocking send: Sender waits until message is received
> - Blocking receive: Receiver waits until message is available
> 
> **Non-Blocking (Asynchronous):**
> - Non-blocking send: Sender sends and continues
> - Non-blocking receive: Receiver gets message or null/error

---

**Q101.** Compare Message Passing and Shared Memory?
> **Answer:**
> | Aspect | Message Passing | Shared Memory |
> |--------|-----------------|---------------|
> | **Communication** | Explicit send/receive | Read/write to shared location |
> | **Synchronization** | Built into send/receive | Requires semaphores/monitors |
> | **Speed** | Slower (copying overhead) | Faster (direct access) |
> | **Distributed Systems** | Works across network | Local only |
> | **Protection** | Automatic isolation | Requires careful programming |

---

**Q102.** What are Event Counters?
> **Answer:** Event Counters are a synchronization mechanism where an event counter is an integer variable that can only increase (monotonically increasing).
> 
> **Properties:** Initial value is 0, never decreases, always ≥ 0
> 
> **Operations:**
> - `read(E)`: Returns current value
> - `advance(E)`: Atomically increments E by 1
> - `await(E, v)`: Blocks until E ≥ v

---

**Q103.** Compare all Synchronization Mechanisms?
> **Answer:**
> | Mechanism | Level | Key Feature | Use Case |
> |-----------|-------|-------------|----------|
> | **Disable Interrupts** | Hardware | Simplest, kernel only | Very short critical sections |
> | **TSL / CAS** | Hardware | Atomic test-and-set | Low-level locks |
> | **Semaphores** | OS | wait() / signal() | General synchronization |
> | **Monitors** | Language | Automatic mutex | High-level, OOP |
> | **Message Passing** | OS/Network | send() / receive() | Distributed systems |
> | **Event Counters** | OS | Monotonic counter | Order-based sync |

---

**Q104.** Compare Semaphores, Monitors, and Message Passing?
> **Answer:**
> | Feature | Semaphore | Monitor | Message Passing |
> |---------|-----------|---------|-----------------|
> | **Mutual Exclusion** | Manual (wait/signal) | Automatic | Via message order |
> | **Synchronization** | Manual | Condition variables | Blocking send/receive |
> | **Error-prone** | Yes | Less | Less |
> | **Distributed** | No | No | Yes |
> | **Language Support** | OS primitive | Java, C# | MPI, Sockets |

